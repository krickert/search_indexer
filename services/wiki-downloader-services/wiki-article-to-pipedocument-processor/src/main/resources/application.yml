micronaut:
  application:
    name: wiki-downloader
  executors:
    consumer:
      type: fixed
      nThreads: 40
    producer:
      type: fixed
      nThreads: 40
      
consul:
  client:
    registration:
      enabled: true
    defaultZone: ${CONSUL_HOST:localhost}:${CONSUL_PORT:8500}

kafka:
  enabled: true
  schema.registry.url: http://localhost:8081
  consumers:
    wiki-parsed-article:
      max:
        poll:
          records: 1000
  key:
    serializer: org.apache.kafka.common.serialization.UUIDSerializer
    deserializer: org.apache.kafka.common.serialization.UUIDDeserializer
  value:
    serializer: io.confluent.kafka.serializers.protobuf.KafkaProtobufSerializer
    deserializer: io.confluent.kafka.serializers.protobuf.KafkaProtobufDeserializer

download:
  request-url: https://dumps.wikimedia.org/enwiki/latest/
  threads: 3
  connection-url: https://dumps.wikimedia.org/enwiki/latest/
  max-tries: 3

wikipedia:
  md5-url: https://dumps.wikimedia.org/enwiki/latest/
  download-location: /Volumes/BigDataFun/wikidownloads
  download-name: "${wikipedia.download-location}/wikiList.md5"
  prefix-url: https://dumps.wikimedia.org/enwiki/
  fresh-copy: true
  parse-only-articles: true               #avoid redirects or other garbage
  add-wiki-text: false
  paragraph-strategy: section

pipeline:
  queue:
    capacity: 100000
    core-pool-size: 1
    poll-time-seconds: 4
    delay-in-seconds: 2
